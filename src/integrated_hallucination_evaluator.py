"""
é›†æˆå¹»è§‰è¯„ä¼°å™¨
æ”¯æŒHHEMå’Œé€šä¹‰åƒé—®ä¸¤ç§è¯„ä¼°æ–¹å¼çš„ç»Ÿä¸€æ¥å£
"""

import asyncio
from typing import List, Dict, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import statistics

from .HHEM_API import HHEMFactualConsistencyAPI, HHEMResponse
from .qwen_hallucination_evaluator import QwenHallucinationEvaluator, QwenHallucinationResponse, QwenModel


class EvaluationMethod(Enum):
    """è¯„ä¼°æ–¹æ³•æšä¸¾"""
    HHEM_ONLY = "hhem_only"  # ä»…ä½¿ç”¨HHEM
    QWEN_ONLY = "qwen_only"  # ä»…ä½¿ç”¨é€šä¹‰åƒé—®
    BOTH = "both"  # åŒæ—¶ä½¿ç”¨ä¸¤ç§æ–¹æ³•
    ENSEMBLE = "ensemble"  # é›†æˆè¯„ä¼°ï¼ˆç»¼åˆä¸¤ç§æ–¹æ³•çš„ç»“æœï¼‰


@dataclass
class IntegratedEvaluationResult:
    """é›†æˆè¯„ä¼°ç»“æœ"""
    # å…ƒä¿¡æ¯ï¼ˆå¿…éœ€å‚æ•°æ”¾åœ¨å‰é¢ï¼‰
    method_used: EvaluationMethod
    success: bool = False
    
    # HHEMç»“æœ
    hhem_score: Optional[float] = None
    hhem_success: bool = False
    hhem_interpretation: Optional[str] = None
    
    # Qwenç»“æœ  
    qwen_hallucination_score: Optional[float] = None
    qwen_confidence: Optional[float] = None
    qwen_success: bool = False
    qwen_explanation: Optional[str] = None
    qwen_interpretation: Optional[str] = None
    
    # é›†æˆç»“æœ
    ensemble_score: Optional[float] = None  # ç»¼åˆè¯„ä¼°åˆ†æ•° (0-1ï¼Œè¶Šä½è¶Šå¥½)
    ensemble_confidence: Optional[float] = None  # é›†æˆç½®ä¿¡åº¦
    ensemble_interpretation: Optional[str] = None
    
    # é”™è¯¯ä¿¡æ¯
    error_messages: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.error_messages is None:
            self.error_messages = []


class IntegratedHallucinationEvaluator:
    """
    é›†æˆå¹»è§‰è¯„ä¼°å™¨
    
    æ”¯æŒä½¿ç”¨HHEMå’Œé€šä¹‰åƒé—®ä¸¤ç§æ–¹æ³•è¿›è¡Œå¹»è§‰è¯„ä¼°ï¼Œ
    å¹¶æä¾›é›†æˆè¯„ä¼°åŠŸèƒ½ä»¥è·å¾—æ›´å¯é çš„ç»“æœ
    """
    
    def __init__(
        self, 
        vectara_api_key: Optional[str] = None,
        dashscope_api_key: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–é›†æˆè¯„ä¼°å™¨
        
        Args:
            vectara_api_key: Vectara APIå¯†é’¥
            dashscope_api_key: é˜¿é‡Œäº‘DashScope APIå¯†é’¥
        """
        self.hhem_evaluator = None
        self.qwen_evaluator = None
        
        # åˆå§‹åŒ–HHEMè¯„ä¼°å™¨
        if vectara_api_key:
            try:
                self.hhem_evaluator = HHEMFactualConsistencyAPI(api_key=vectara_api_key)
            except Exception as e:
                print(f"è­¦å‘Š: HHEMè¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–Qwenè¯„ä¼°å™¨
        if dashscope_api_key:
            try:
                self.qwen_evaluator = QwenHallucinationEvaluator(api_key=dashscope_api_key)
            except Exception as e:
                print(f"è­¦å‘Š: Qwenè¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        if not self.hhem_evaluator and not self.qwen_evaluator:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªæœ‰æ•ˆçš„APIå¯†é’¥æ¥åˆå§‹åŒ–è¯„ä¼°å™¨")
    
    def evaluate(
        self,
        generated_text: str,
        source_texts: List[str],
        method: EvaluationMethod = EvaluationMethod.ENSEMBLE,
        qwen_model: QwenModel = QwenModel.QWEN_TURBO,
        hhem_model: str = "hhem_v2.3"
    ) -> IntegratedEvaluationResult:
        """
        æ‰§è¡Œé›†æˆå¹»è§‰è¯„ä¼°
        
        Args:
            generated_text: å¾…è¯„ä¼°çš„ç”Ÿæˆæ–‡æœ¬
            source_texts: å‚è€ƒæºæ–‡æœ¬åˆ—è¡¨
            method: è¯„ä¼°æ–¹æ³•
            qwen_model: Qwenæ¨¡å‹ç±»å‹
            hhem_model: HHEMæ¨¡å‹åç§°
            
        Returns:
            IntegratedEvaluationResult: é›†æˆè¯„ä¼°ç»“æœ
        """
        result = IntegratedEvaluationResult(method_used=method)
        
        # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ‰§è¡Œè¯„ä¼°
        if method in [EvaluationMethod.HHEM_ONLY, EvaluationMethod.BOTH, EvaluationMethod.ENSEMBLE]:
            if self.hhem_evaluator:
                hhem_result = self._evaluate_hhem(generated_text, source_texts, hhem_model)
                result.hhem_score = hhem_result.score if hhem_result.success else None
                result.hhem_success = hhem_result.success
                result.hhem_interpretation = (
                    self.hhem_evaluator.interpret_score(hhem_result.score) 
                    if hhem_result.success else None
                )
                if not hhem_result.success:
                    result.error_messages.append(f"HHEMè¯„ä¼°å¤±è´¥: {hhem_result.error_message}")
            else:
                result.error_messages.append("HHEMè¯„ä¼°å™¨æœªåˆå§‹åŒ–")
        
        if method in [EvaluationMethod.QWEN_ONLY, EvaluationMethod.BOTH, EvaluationMethod.ENSEMBLE]:
            if self.qwen_evaluator:
                qwen_result = self._evaluate_qwen(generated_text, source_texts, qwen_model)
                result.qwen_hallucination_score = qwen_result.hallucination_score if qwen_result.success else None
                result.qwen_confidence = qwen_result.confidence if qwen_result.success else None
                result.qwen_success = qwen_result.success
                result.qwen_explanation = qwen_result.explanation if qwen_result.success else None
                result.qwen_interpretation = (
                    self.qwen_evaluator.interpret_score(qwen_result.hallucination_score)
                    if qwen_result.success else None
                )
                if not qwen_result.success:
                    result.error_messages.append(f"Qwenè¯„ä¼°å¤±è´¥: {qwen_result.error_message}")
            else:
                result.error_messages.append("Qwenè¯„ä¼°å™¨æœªåˆå§‹åŒ–")
        
        # è®¡ç®—é›†æˆç»“æœ
        if method == EvaluationMethod.ENSEMBLE:
            self._compute_ensemble_result(result)
        
        # è®¾ç½®æ€»ä½“æˆåŠŸçŠ¶æ€
        result.success = (
            (result.hhem_success and method in [EvaluationMethod.HHEM_ONLY]) or
            (result.qwen_success and method in [EvaluationMethod.QWEN_ONLY]) or
            (method in [EvaluationMethod.BOTH, EvaluationMethod.ENSEMBLE] and 
             (result.hhem_success or result.qwen_success))
        )
        
        return result
    
    def _evaluate_hhem(self, generated_text: str, source_texts: List[str], model: str) -> HHEMResponse:
        """æ‰§è¡ŒHHEMè¯„ä¼°"""
        return self.hhem_evaluator.evaluate_consistency(
            generated_text=generated_text,
            source_texts=source_texts,
            model_name=model
        )
    
    def _evaluate_qwen(
        self, 
        generated_text: str, 
        source_texts: List[str], 
        model: QwenModel
    ) -> QwenHallucinationResponse:
        """æ‰§è¡ŒQwenè¯„ä¼°"""
        return self.qwen_evaluator.evaluate_hallucination(
            generated_text=generated_text,
            source_texts=source_texts,
            model=model
        )
    
    def _compute_ensemble_result(self, result: IntegratedEvaluationResult):
        """
        è®¡ç®—é›†æˆè¯„ä¼°ç»“æœ
        
        Args:
            result: è¦æ›´æ–°çš„ç»“æœå¯¹è±¡
        """
        scores = []
        confidences = []
        interpretations = []
        
        # æ”¶é›†æœ‰æ•ˆåˆ†æ•°
        if result.hhem_success and result.hhem_score is not None:
            # HHEMåˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¶Šä¸€è‡´ï¼Œè½¬æ¢ä¸ºå¹»è§‰åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            hallucination_score = 1.0 - result.hhem_score
            scores.append(hallucination_score)
            confidences.append(0.9)  # HHEMç½®ä¿¡åº¦å‡è®¾ä¸º0.9
            interpretations.append(f"HHEM: {result.hhem_interpretation}")
        
        if result.qwen_success and result.qwen_hallucination_score is not None:
            scores.append(result.qwen_hallucination_score)
            confidences.append(result.qwen_confidence or 0.7)
            interpretations.append(f"Qwen: {result.qwen_interpretation}")
        
        # è®¡ç®—é›†æˆåˆ†æ•°
        if scores:
            # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œæƒé‡åŸºäºç½®ä¿¡åº¦
            if len(scores) > 1:
                weights = confidences
                weighted_sum = sum(s * w for s, w in zip(scores, weights))
                weight_sum = sum(weights)
                result.ensemble_score = weighted_sum / weight_sum if weight_sum > 0 else statistics.mean(scores)
                result.ensemble_confidence = statistics.mean(confidences)
            else:
                result.ensemble_score = scores[0]
                result.ensemble_confidence = confidences[0]
            
            result.ensemble_interpretation = self._interpret_ensemble_score(
                result.ensemble_score, 
                len(scores)
            )
    
    def _interpret_ensemble_score(self, score: float, num_evaluators: int) -> str:
        """è§£é‡Šé›†æˆè¯„ä¼°åˆ†æ•°"""
        base_interpretation = ""
        if score >= 0.8:
            base_interpretation = "ä¸¥é‡å¹»è§‰é£é™© - ç”Ÿæˆæ–‡æœ¬å­˜åœ¨æ˜æ˜¾çš„äº‹å®é”™è¯¯æˆ–è™šå‡ä¿¡æ¯"
        elif score >= 0.6:
            base_interpretation = "ä¸­ç­‰å¹»è§‰é£é™© - ç”Ÿæˆæ–‡æœ¬å­˜åœ¨ä¸€äº›ä¸å‡†ç¡®æˆ–å¯ç–‘çš„å†…å®¹"
        elif score >= 0.4:
            base_interpretation = "è½»å¾®å¹»è§‰é£é™© - ç”Ÿæˆæ–‡æœ¬å¤§éƒ¨åˆ†å‡†ç¡®ï¼Œä½†å­˜åœ¨ç»†èŠ‚é—®é¢˜"
        elif score >= 0.2:
            base_interpretation = "ä½å¹»è§‰é£é™© - ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æ¡£åŸºæœ¬ä¸€è‡´"
        else:
            base_interpretation = "æä½å¹»è§‰é£é™© - ç”Ÿæˆæ–‡æœ¬é«˜åº¦å‡†ç¡®å’Œå¯ä¿¡"
        
        evaluator_info = f"(åŸºäº{num_evaluators}ä¸ªè¯„ä¼°å™¨çš„é›†æˆç»“æœ)"
        return f"{base_interpretation} {evaluator_info}"
    
    def batch_evaluate(
        self,
        evaluations: List[Dict[str, Union[str, List[str]]]],
        method: EvaluationMethod = EvaluationMethod.ENSEMBLE,
        qwen_model: QwenModel = QwenModel.QWEN_TURBO,
        hhem_model: str = "hhem_v2.3"
    ) -> List[IntegratedEvaluationResult]:
        """
        æ‰¹é‡è¯„ä¼°
        
        Args:
            evaluations: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
            method: è¯„ä¼°æ–¹æ³•
            qwen_model: Qwenæ¨¡å‹ç±»å‹
            hhem_model: HHEMæ¨¡å‹åç§°
            
        Returns:
            List[IntegratedEvaluationResult]: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        results = []
        for i, eval_data in enumerate(evaluations):
            try:
                generated_text = eval_data.get('generated_text', '')
                source_texts = eval_data.get('source_texts', [])
                
                result = self.evaluate(
                    generated_text=generated_text,
                    source_texts=source_texts,
                    method=method,
                    qwen_model=qwen_model,
                    hhem_model=hhem_model
                )
                results.append(result)
                
            except Exception as e:
                error_result = IntegratedEvaluationResult(
                    method_used=method,
                    success=False,
                    error_messages=[f"æ‰¹é‡è¯„ä¼°ç¬¬{i+1}é¡¹é”™è¯¯: {str(e)}"]
                )
                results.append(error_result)
        
        return results
    
    def compare_methods(
        self,
        generated_text: str,
        source_texts: List[str],
        qwen_model: QwenModel = QwenModel.QWEN_TURBO,
        hhem_model: str = "hhem_v2.3"
    ) -> Dict[str, IntegratedEvaluationResult]:
        """
        æ¯”è¾ƒä¸åŒè¯„ä¼°æ–¹æ³•çš„ç»“æœ
        
        Args:
            generated_text: å¾…è¯„ä¼°æ–‡æœ¬
            source_texts: å‚è€ƒæ–‡æ¡£
            qwen_model: Qwenæ¨¡å‹
            hhem_model: HHEMæ¨¡å‹
            
        Returns:
            Dict[str, IntegratedEvaluationResult]: å„ç§æ–¹æ³•çš„è¯„ä¼°ç»“æœ
        """
        results = {}
        
        methods = [EvaluationMethod.HHEM_ONLY, EvaluationMethod.QWEN_ONLY, EvaluationMethod.ENSEMBLE]
        
        for method in methods:
            try:
                result = self.evaluate(
                    generated_text=generated_text,
                    source_texts=source_texts,
                    method=method,
                    qwen_model=qwen_model,
                    hhem_model=hhem_model
                )
                results[method.value] = result
            except Exception as e:
                results[method.value] = IntegratedEvaluationResult(
                    method_used=method,
                    success=False,
                    error_messages=[f"æ–¹æ³•{method.value}è¯„ä¼°å¤±è´¥: {str(e)}"]
                )
        
        return results


def demo_usage():
    """æ¼”ç¤ºé›†æˆè¯„ä¼°å™¨çš„ä½¿ç”¨"""
    # æ³¨æ„ï¼šéœ€è¦è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡æˆ–æä¾›APIå¯†é’¥
    try:
        # åˆå§‹åŒ–é›†æˆè¯„ä¼°å™¨ï¼ˆä½¿ç”¨ä½ çš„APIå¯†é’¥ï¼‰
        evaluator = IntegratedHallucinationEvaluator(
            vectara_api_key="zut_aE6JAgOK4H3CaDxnaz4rEfq8fI9FrYOJr_Es2g",  # ä½ çš„HHEM APIå¯†é’¥
            dashscope_api_key=None  # éœ€è¦è®¾ç½®ä½ çš„DashScope APIå¯†é’¥
        )
        
        print("=== é›†æˆå¹»è§‰è¯„ä¼°å™¨æ¼”ç¤º ===\n")
        
        test_case = {
            "generated_text": "å·´é»é“å¡”ä½äºä¼¦æ•¦ï¼Œæ˜¯è‹±å›½è‘—åæ™¯ç‚¹ï¼Œé«˜åº¦ä¸º324ç±³ã€‚",
            "source_texts": [
                "åŸƒè²å°”é“å¡”ï¼ˆEiffel Towerï¼‰åè½äºæ³•å›½å·´é»ï¼Œæ˜¯æ³•å›½æœ€è‘—åçš„åœ°æ ‡ä¹‹ä¸€ï¼Œé«˜åº¦ä¸º324ç±³ã€‚",
                "ä¼¦æ•¦æ˜¯è‹±å›½çš„é¦–éƒ½ï¼Œè€Œå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ã€‚"
            ]
        }
        
        # 1. å•ä¸€æ–¹æ³•è¯„ä¼°
        print("1. ä½¿ç”¨HHEMè¯„ä¼°ï¼š")
        hhem_result = evaluator.evaluate(
            generated_text=test_case["generated_text"],
            source_texts=test_case["source_texts"],
            method=EvaluationMethod.HHEM_ONLY
        )
        
        if hhem_result.success:
            print(f"   âœ… HHEMåˆ†æ•°: {hhem_result.hhem_score:.4f}")
            print(f"   ğŸ“ è§£é‡Š: {hhem_result.hhem_interpretation}")
        else:
            print(f"   âŒ å¤±è´¥: {', '.join(hhem_result.error_messages)}")
        
        print("\n" + "-"*50 + "\n")
        
        # 2. æ–¹æ³•æ¯”è¾ƒ
        print("2. æ–¹æ³•æ¯”è¾ƒï¼š")
        comparison = evaluator.compare_methods(
            generated_text=test_case["generated_text"],
            source_texts=test_case["source_texts"]
        )
        
        for method_name, result in comparison.items():
            print(f"\n{method_name.upper()}:")
            if result.success:
                if result.hhem_score is not None:
                    print(f"   HHEMåˆ†æ•°: {result.hhem_score:.4f}")
                if result.qwen_hallucination_score is not None:
                    print(f"   Qwenå¹»è§‰åˆ†æ•°: {result.qwen_hallucination_score:.4f}")
                if result.ensemble_score is not None:
                    print(f"   é›†æˆåˆ†æ•°: {result.ensemble_score:.4f}")
                    print(f"   é›†æˆç½®ä¿¡åº¦: {result.ensemble_confidence:.4f}")
                    print(f"   é›†æˆè§£é‡Š: {result.ensemble_interpretation}")
            else:
                print(f"   âŒ å¤±è´¥: {', '.join(result.error_messages)}")
        
        print("\n" + "="*50 + "\n")
        
        # 3. æ‰¹é‡è¯„ä¼°æ¼”ç¤º
        print("3. æ‰¹é‡è¯„ä¼°æ¼”ç¤ºï¼š")
        batch_data = [
            {
                "generated_text": "è‹¹æœæ˜¯ä¸€ç§è“è‰²çš„æ°´æœï¼Œå¯Œå«ç»´ç”Ÿç´ C",
                "source_texts": ["è‹¹æœé€šå¸¸æ˜¯çº¢è‰²ã€ç»¿è‰²æˆ–é»„è‰²çš„æ°´æœï¼Œå¯Œå«ç»´ç”Ÿç´ Cå’Œçº¤ç»´"]
            },
            {
                "generated_text": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£çº¦2100ä¸‡",
                "source_texts": ["ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½æ˜¯åŒ—äº¬å¸‚ï¼Œå¸¸ä½äººå£çº¦2100ä¸‡"]
            }
        ]
        
        batch_results = evaluator.batch_evaluate(
            evaluations=batch_data,
            method=EvaluationMethod.HHEM_ONLY  # ç”±äºå¯èƒ½åªæœ‰HHEMå¯ç”¨
        )
        
        for i, result in enumerate(batch_results, 1):
            print(f"\næ‰¹é‡è¯„ä¼°ä»»åŠ¡ {i}:")
            if result.success:
                if result.hhem_score is not None:
                    print(f"   HHEMåˆ†æ•°: {result.hhem_score:.4f} - {result.hhem_interpretation}")
                if result.ensemble_score is not None:
                    print(f"   é›†æˆåˆ†æ•°: {result.ensemble_score:.4f} - {result.ensemble_interpretation}")
            else:
                print(f"   âŒ å¤±è´¥: {', '.join(result.error_messages)}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿ï¼š")
        print("1. å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…")
        print("2. å·²è®¾ç½®æ­£ç¡®çš„APIå¯†é’¥")
        print("3. ç½‘ç»œè¿æ¥æ­£å¸¸")


if __name__ == "__main__":
    demo_usage()
