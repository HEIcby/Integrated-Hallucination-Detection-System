"""
RAGtruth å¤§è§„æ¨¡è¯„ä¼°è„šæœ¬
åœ¨ RAGtruth æ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢çš„ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import time
import statistics
from typing import List, Dict, Tuple
from dataclasses import dataclass
import argparse

from src.ragtruth_loader import RAGtruthLoader, TaskType, SplitType, RAGtruthSample
from src.integrated_hallucination_evaluator import IntegratedHallucinationEvaluator, EvaluationMethod
from src.qwen_hallucination_evaluator import QwenModel


@dataclass
class EvaluationResults:
    """è¯„ä¼°ç»“æœæ•°æ®ç±»"""
    total_samples: int = 0
    successful_evaluations: int = 0
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    
    # æ··æ·†çŸ©é˜µ
    true_positives: int = 0
    false_positives: int = 0
    true_negatives: int = 0
    false_negatives: int = 0
    
    # åˆ†æ•°ç»Ÿè®¡
    hallucination_scores: List[float] = None
    non_hallucination_scores: List[float] = None
    
    # æ—¶é—´ç»Ÿè®¡
    evaluation_time: float = 0.0
    
    def __post_init__(self):
        if self.hallucination_scores is None:
            self.hallucination_scores = []
        if self.non_hallucination_scores is None:
            self.non_hallucination_scores = []


class LargeScaleEvaluator:
    """å¤§è§„æ¨¡è¯„ä¼°å™¨"""
    
    def __init__(self, vectara_api_key: str = None, dashscope_api_key: str = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            vectara_api_key: Vectara APIå¯†é’¥
            dashscope_api_key: é˜¿é‡Œäº‘DashScope APIå¯†é’¥
        """
        self.loader = RAGtruthLoader()
        self.evaluator = IntegratedHallucinationEvaluator(
            vectara_api_key=vectara_api_key,
            dashscope_api_key=dashscope_api_key
        )
        self.hallucination_threshold = 0.5  # HHEMé˜ˆå€¼
        self.qwen_threshold = 0.2  # Qwené˜ˆå€¼
    
    def evaluate_samples(
        self,
        samples: List[RAGtruthSample],
        method: EvaluationMethod = EvaluationMethod.HHEM_ONLY,
        show_progress: bool = True
    ) -> EvaluationResults:
        """
        è¯„ä¼°æ ·æœ¬åˆ—è¡¨
        
        Args:
            samples: è¦è¯„ä¼°çš„æ ·æœ¬åˆ—è¡¨
            method: è¯„ä¼°æ–¹æ³•
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            EvaluationResults: è¯„ä¼°ç»“æœ
        """
        results = EvaluationResults()
        results.total_samples = len(samples)
        
        start_time = time.time()
        
        if show_progress:
            print(f"ğŸ” å¼€å§‹è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬...")
            print("è¿›åº¦: ", end="", flush=True)
        
        individual_results = []
        
        for i, sample in enumerate(samples):
            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and (i % max(1, len(samples) // 20) == 0 or i == len(samples) - 1):
                progress = (i + 1) / len(samples) * 100
                print(f"\rè¿›åº¦: {progress:.1f}% ({i+1}/{len(samples)})", end="", flush=True)
            
            # è·³è¿‡æºä¿¡æ¯è¿‡çŸ­çš„æ ·æœ¬
            source_info_str = str(sample.source.source_info)
            if len(source_info_str.strip()) < 10:
                continue
            
            try:
                # æ‰§è¡Œè¯„ä¼°
                result = self.evaluator.evaluate(
                    generated_text=sample.generated_text,
                    source_texts=sample.source_texts,
                    method=method
                )
                
                if result.success:
                    results.successful_evaluations += 1
                    
                    # è·å–ç›¸åº”çš„åˆ†æ•°
                    score = None
                    if method == EvaluationMethod.HHEM_ONLY and result.hhem_score is not None:
                        score = result.hhem_score
                        predicted_hallucination = score < self.hallucination_threshold
                    elif method == EvaluationMethod.QWEN_ONLY and result.qwen_hallucination_score is not None:
                        score = result.qwen_hallucination_score
                        predicted_hallucination = score > self.qwen_threshold
                    elif method in [EvaluationMethod.BOTH, EvaluationMethod.ENSEMBLE] and result.ensemble_score is not None:
                        score = result.ensemble_score
                        predicted_hallucination = score > self.hallucination_threshold
                    else:
                        continue
                    
                    actual_hallucination = sample.has_hallucination
                    
                    # æ›´æ–°æ··æ·†çŸ©é˜µ
                    if predicted_hallucination and actual_hallucination:
                        results.true_positives += 1
                    elif predicted_hallucination and not actual_hallucination:
                        results.false_positives += 1
                    elif not predicted_hallucination and not actual_hallucination:
                        results.true_negatives += 1
                    elif not predicted_hallucination and actual_hallucination:
                        results.false_negatives += 1
                    
                    # æ”¶é›†åˆ†æ•°
                    if actual_hallucination:
                        results.hallucination_scores.append(score)
                    else:
                        results.non_hallucination_scores.append(score)
                    
                    individual_results.append({
                        'predicted': predicted_hallucination,
                        'actual': actual_hallucination,
                        'score': score,
                        'correct': predicted_hallucination == actual_hallucination
                    })
                    
            except Exception as e:
                if show_progress and i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    print(f"\nâš ï¸ æ ·æœ¬ {i+1} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        results.evaluation_time = time.time() - start_time
        
        # è®¡ç®—æŒ‡æ ‡
        if results.successful_evaluations > 0:
            total_predictions = results.true_positives + results.false_positives + results.true_negatives + results.false_negatives
            
            if total_predictions > 0:
                results.accuracy = (results.true_positives + results.true_negatives) / total_predictions
            
            if results.true_positives + results.false_positives > 0:
                results.precision = results.true_positives / (results.true_positives + results.false_positives)
            
            if results.true_positives + results.false_negatives > 0:
                results.recall = results.true_positives / (results.true_positives + results.false_negatives)
            
            if results.precision + results.recall > 0:
                results.f1_score = 2 * (results.precision * results.recall) / (results.precision + results.recall)
        
        if show_progress:
            print()  # æ¢è¡Œ
        
        return results
    
    def comprehensive_evaluation(
        self,
        max_samples: int = 100,
        task_types: List[TaskType] = None,
        methods: List[EvaluationMethod] = None
    ):
        """
        å…¨é¢è¯„ä¼°
        
        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            task_types: è¦æµ‹è¯•çš„ä»»åŠ¡ç±»å‹åˆ—è¡¨
            methods: è¦æµ‹è¯•çš„è¯„ä¼°æ–¹æ³•åˆ—è¡¨
        """
        if task_types is None:
            task_types = [TaskType.ALL]
        
        if methods is None:
            methods = [EvaluationMethod.HHEM_ONLY]
        
        print("ğŸš€ RAGtruth å¤§è§„æ¨¡è¯„ä¼°")
        print("=" * 60)
        
        all_results = {}
        
        for task_type in task_types:
            print(f"\nğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type.value}")
            print("-" * 40)
            
            # è·å–æ ·æœ¬
            samples = self.loader.get_samples(
                task_type=task_type,
                split=SplitType.TEST,
                max_samples=max_samples,
                random_seed=42
            )
            
            print(f"ğŸ“Š è·å–åˆ° {len(samples)} ä¸ªæ ·æœ¬")
            
            for method in methods:
                print(f"\nğŸ” è¯„ä¼°æ–¹æ³•: {method.value}")
                
                results = self.evaluate_samples(samples, method)
                all_results[f"{task_type.value}_{method.value}"] = results
                
                self.print_results(results)
        
        # æ€»ç»“å¯¹æ¯”
        if len(all_results) > 1:
            self.print_comparison(all_results)
    
    def print_results(self, results: EvaluationResults):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"   âœ… æˆåŠŸè¯„ä¼°: {results.successful_evaluations}/{results.total_samples} æ ·æœ¬")
        print(f"   ğŸ¯ å‡†ç¡®ç‡: {results.accuracy:.3f}")
        print(f"   ğŸ“Š ç²¾ç¡®ç‡: {results.precision:.3f}")
        print(f"   ğŸ“Š å¬å›ç‡: {results.recall:.3f}")
        print(f"   ğŸ“Š F1åˆ†æ•°: {results.f1_score:.3f}")
        print(f"   â±ï¸ è¯„ä¼°æ—¶é—´: {results.evaluation_time:.1f}ç§’")
        
        print(f"   ğŸ“ˆ æ··æ·†çŸ©é˜µ:")
        print(f"      TP: {results.true_positives}, FP: {results.false_positives}")
        print(f"      TN: {results.true_negatives}, FN: {results.false_negatives}")
        
        if results.hallucination_scores and results.non_hallucination_scores:
            halluc_mean = statistics.mean(results.hallucination_scores)
            non_halluc_mean = statistics.mean(results.non_hallucination_scores)
            print(f"   ğŸ“Š æœ‰å¹»è§‰æ ·æœ¬å¹³å‡åˆ†æ•°: {halluc_mean:.4f} (n={len(results.hallucination_scores)})")
            print(f"   ğŸ“Š æ— å¹»è§‰æ ·æœ¬å¹³å‡åˆ†æ•°: {non_halluc_mean:.4f} (n={len(results.non_hallucination_scores)})")
    
    def print_comparison(self, all_results: Dict[str, EvaluationResults]):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print(f"\nğŸ† è¯„ä¼°ç»“æœå¯¹æ¯”")
        print("=" * 60)
        
        print(f"{'é…ç½®':<20} {'å‡†ç¡®ç‡':<8} {'F1åˆ†æ•°':<8} {'æˆåŠŸç‡':<8} {'æ—¶é—´(ç§’)':<10}")
        print("-" * 60)
        
        for config, results in all_results.items():
            success_rate = results.successful_evaluations / results.total_samples
            print(f"{config:<20} {results.accuracy:<8.3f} {results.f1_score:<8.3f} {success_rate:<8.3f} {results.evaluation_time:<10.1f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='RAGtruth å¤§è§„æ¨¡è¯„ä¼°')
    parser.add_argument('--samples', type=int, default=100,
                        help='æœ€å¤§æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)')
    parser.add_argument('--task', choices=['ALL', 'Summary', 'QA', 'Data2txt'], 
                        default='ALL', help='ä»»åŠ¡ç±»å‹ (é»˜è®¤: ALL)')
    parser.add_argument('--method', choices=['HHEM_ONLY', 'QWEN_ONLY', 'ENSEMBLE'], 
                        default='HHEM_ONLY', help='è¯„ä¼°æ–¹æ³• (é»˜è®¤: HHEM_ONLY)')
    parser.add_argument('--vectara-key', type=str, 
                        default="zut_aE6JAgOK4H3CaDxnaz4rEfq8fI9FrYOJr_Es2g",
                        help='Vectara APIå¯†é’¥')
    parser.add_argument('--dashscope-key', type=str, 
                        help='DashScope APIå¯†é’¥')
    parser.add_argument('--comprehensive', action='store_true',
                        help='è¿è¡Œå…¨é¢å¯¹æ¯”è¯„ä¼°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LargeScaleEvaluator(
        vectara_api_key=args.vectara_key,
        dashscope_api_key=args.dashscope_key
    )
    
    if args.comprehensive:
        # å…¨é¢è¯„ä¼°
        task_types = [TaskType.ALL, TaskType.SUMMARY, TaskType.QA]
        methods = [EvaluationMethod.HHEM_ONLY]
        
        if args.dashscope_key:
            methods.extend([EvaluationMethod.QWEN_ONLY, EvaluationMethod.ENSEMBLE])
        
        evaluator.comprehensive_evaluation(
            max_samples=args.samples,
            task_types=task_types,
            methods=methods
        )
    else:
        # å•ä¸ªé…ç½®è¯„ä¼°
        task_type = getattr(TaskType, args.task)
        method = getattr(EvaluationMethod, args.method)
        
        samples = evaluator.loader.get_samples(
            task_type=task_type,
            split=SplitType.TEST,
            max_samples=args.samples,
            random_seed=42
        )
        
        print(f"ğŸ” è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬ - ä»»åŠ¡: {args.task}, æ–¹æ³•: {args.method}")
        results = evaluator.evaluate_samples(samples, method)
        evaluator.print_results(results)


if __name__ == "__main__":
    main()
