"""
RAGtruth æ•°æ®é›†é›†æˆè¯„æµ‹ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ RAGtruth æ•°æ®é›†å¯¹é›†æˆå¹»è§‰è¯„ä¼°å™¨è¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import time
import statistics
from typing import List, Dict, Tuple
from dataclasses import dataclass

from src.ragtruth_loader import RAGtruthLoader, TaskType, SplitType, RAGtruthSample
from src.integrated_hallucination_evaluator import IntegratedHallucinationEvaluator, EvaluationMethod
from src.qwen_hallucination_evaluator import QwenModel


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    total_samples: int = 0
    successful_evaluations: int = 0
    
    # å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆä¸äººå·¥æ ‡æ³¨å¯¹æ¯”ï¼‰
    true_positives: int = 0  # æ­£ç¡®è¯†åˆ«çš„å¹»è§‰
    false_positives: int = 0  # è¯¯æŠ¥çš„å¹»è§‰
    true_negatives: int = 0  # æ­£ç¡®è¯†åˆ«çš„éå¹»è§‰
    false_negatives: int = 0  # æ¼æŠ¥çš„å¹»è§‰
    
    # è¯„ä¼°å™¨æ€§èƒ½
    hhem_success_rate: float = 0.0
    qwen_success_rate: float = 0.0
    ensemble_success_rate: float = 0.0
    
    # åˆ†æ•°ç»Ÿè®¡
    hallucination_scores: List[float] = None
    non_hallucination_scores: List[float] = None
    
    def __post_init__(self):
        if self.hallucination_scores is None:
            self.hallucination_scores = []
        if self.non_hallucination_scores is None:
            self.non_hallucination_scores = []
    
    @property
    def accuracy(self) -> float:
        """å‡†ç¡®ç‡"""
        total = self.true_positives + self.false_positives + self.true_negatives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total
    
    @property
    def precision(self) -> float:
        """ç²¾ç¡®ç‡"""
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)
    
    @property
    def recall(self) -> float:
        """å¬å›ç‡"""
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)
    
    @property
    def f1_score(self) -> float:
        """F1åˆ†æ•°"""
        if self.precision + self.recall == 0:
            return 0.0
        return 2 * (self.precision * self.recall) / (self.precision + self.recall)


class RAGtruthEvaluator:
    """RAGtruth æ•°æ®é›†è¯„æµ‹å™¨"""
    
    def __init__(
        self,
        vectara_api_key: str = None,
        dashscope_api_key: str = None,
        dataset_path: str = None
    ):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            vectara_api_key: Vectara APIå¯†é’¥
            dashscope_api_key: é˜¿é‡Œäº‘DashScope APIå¯†é’¥  
            dataset_path: RAGtruth æ•°æ®é›†è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é¡¹ç›®å†…çš„é»˜è®¤è·¯å¾„
        """
        self.loader = RAGtruthLoader(dataset_path)
        self.evaluator = IntegratedHallucinationEvaluator(
            vectara_api_key=vectara_api_key,
            dashscope_api_key=dashscope_api_key
        )
        
        # é˜ˆå€¼è®¾ç½®
        self.hallucination_threshold = 0.5  # è¶…è¿‡æ­¤é˜ˆå€¼è®¤ä¸ºå­˜åœ¨å¹»è§‰
    
    def evaluate_on_dataset(
        self,
        max_samples: int = 100,
        task_type: TaskType = TaskType.ALL,
        split: SplitType = SplitType.TEST,
        evaluation_method: EvaluationMethod = EvaluationMethod.ENSEMBLE,
        random_seed: int = 42
    ) -> EvaluationMetrics:
        """
        åœ¨ RAGtruth æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°
        
        Args:
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
            task_type: ä»»åŠ¡ç±»å‹
            split: æ•°æ®é›†åˆ†å‰²
            evaluation_method: è¯„ä¼°æ–¹æ³•
            random_seed: éšæœºç§å­
            
        Returns:
            EvaluationMetrics: è¯„ä¼°æŒ‡æ ‡
        """
        print(f"ğŸ” å¼€å§‹åœ¨ RAGtruth æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°")
        print(f"ğŸ“Š å‚æ•°è®¾ç½®:")
        print(f"   æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
        print(f"   ä»»åŠ¡ç±»å‹: {task_type.value}")
        print(f"   æ•°æ®é›†åˆ†å‰²: {split.value}")
        print(f"   è¯„ä¼°æ–¹æ³•: {evaluation_method.value}")
        print(f"   å¹»è§‰é˜ˆå€¼: {self.hallucination_threshold}")
        print("=" * 60)
        
        # è·å–æ ·æœ¬
        samples = self.loader.get_samples(
            task_type=task_type,
            split=split,
            max_samples=max_samples,
            random_seed=random_seed
        )
        
        metrics = EvaluationMetrics()
        metrics.total_samples = len(samples)
        
        hhem_successes = 0
        qwen_successes = 0
        ensemble_successes = 0
        
        print(f"ğŸ“ å¼€å§‹è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬...")
        
        for i, sample in enumerate(samples, 1):
            try:
                # æ‰§è¡Œè¯„ä¼°
                result = self.evaluator.evaluate(
                    generated_text=sample.generated_text,
                    source_texts=sample.source_texts,
                    method=evaluation_method
                )
                
                if result.success:
                    metrics.successful_evaluations += 1
                    
                    # ç»Ÿè®¡å„è¯„ä¼°å™¨æˆåŠŸç‡
                    if result.hhem_success:
                        hhem_successes += 1
                    if result.qwen_success:
                        qwen_successes += 1
                    if result.ensemble_score is not None:
                        ensemble_successes += 1
                    
                    # åˆ¤æ–­è¯„ä¼°å™¨é¢„æµ‹ç»“æœ
                    predicted_hallucination = False
                    score_to_use = None
                    
                    if evaluation_method == EvaluationMethod.HHEM_ONLY and result.hhem_score is not None:
                        # HHEM åˆ†æ•°è¶Šä½è¡¨ç¤ºè¶Šä¸€è‡´ï¼ˆæ— å¹»è§‰ï¼‰ï¼Œæ‰€ä»¥é«˜åˆ†æ•°è¡¨ç¤ºå¹»è§‰
                        predicted_hallucination = result.hhem_score > self.hallucination_threshold
                        score_to_use = result.hhem_score
                    elif evaluation_method == EvaluationMethod.QWEN_ONLY and result.qwen_hallucination_score is not None:
                        # Qwen å¹»è§‰åˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¶Šå¯èƒ½æ˜¯å¹»è§‰
                        predicted_hallucination = result.qwen_hallucination_score > self.hallucination_threshold
                        score_to_use = result.qwen_hallucination_score
                    elif evaluation_method in [EvaluationMethod.BOTH, EvaluationMethod.ENSEMBLE] and result.ensemble_score is not None:
                        # é›†æˆåˆ†æ•°
                        predicted_hallucination = result.ensemble_score > self.hallucination_threshold
                        score_to_use = result.ensemble_score
                    
                    # å®é™…æ ‡æ³¨ç»“æœ
                    actual_hallucination = sample.has_hallucination
                    
                    # è®¡ç®—æ··æ·†çŸ©é˜µ
                    if predicted_hallucination and actual_hallucination:
                        metrics.true_positives += 1
                    elif predicted_hallucination and not actual_hallucination:
                        metrics.false_positives += 1
                    elif not predicted_hallucination and not actual_hallucination:
                        metrics.true_negatives += 1
                    elif not predicted_hallucination and actual_hallucination:
                        metrics.false_negatives += 1
                    
                    # æ”¶é›†åˆ†æ•°ç»Ÿè®¡
                    if score_to_use is not None:
                        if actual_hallucination:
                            metrics.hallucination_scores.append(score_to_use)
                        else:
                            metrics.non_hallucination_scores.append(score_to_use)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == len(samples):
                    print(f"   è¿›åº¦: {i}/{len(samples)} ({i/len(samples)*100:.1f}%)")
                
            except Exception as e:
                print(f"   æ ·æœ¬ {i} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æˆåŠŸç‡
        if metrics.total_samples > 0:
            metrics.hhem_success_rate = hhem_successes / metrics.total_samples
            metrics.qwen_success_rate = qwen_successes / metrics.total_samples  
            metrics.ensemble_success_rate = ensemble_successes / metrics.total_samples
        
        return metrics
    
    def print_evaluation_results(self, metrics: EvaluationMetrics):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š RAGtruth æ•°æ®é›†è¯„ä¼°ç»“æœ")
        print("=" * 60)
        
        print(f"ğŸ“ åŸºç¡€ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {metrics.total_samples}")
        print(f"   æˆåŠŸè¯„ä¼°: {metrics.successful_evaluations} ({metrics.successful_evaluations/metrics.total_samples*100:.1f}%)")
        
        print(f"\nğŸ¯ åˆ†ç±»æ€§èƒ½:")
        print(f"   å‡†ç¡®ç‡ (Accuracy): {metrics.accuracy:.4f}")
        print(f"   ç²¾ç¡®ç‡ (Precision): {metrics.precision:.4f}")
        print(f"   å¬å›ç‡ (Recall): {metrics.recall:.4f}")
        print(f"   F1åˆ†æ•°: {metrics.f1_score:.4f}")
        
        print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µ:")
        print(f"   çœŸé˜³æ€§ (TP): {metrics.true_positives}")
        print(f"   å‡é˜³æ€§ (FP): {metrics.false_positives}")
        print(f"   çœŸé˜´æ€§ (TN): {metrics.true_negatives}")
        print(f"   å‡é˜´æ€§ (FN): {metrics.false_negatives}")
        
        print(f"\nğŸ”§ è¯„ä¼°å™¨æˆåŠŸç‡:")
        print(f"   HHEMæˆåŠŸç‡: {metrics.hhem_success_rate:.4f}")
        print(f"   QwenæˆåŠŸç‡: {metrics.qwen_success_rate:.4f}")
        print(f"   é›†æˆæˆåŠŸç‡: {metrics.ensemble_success_rate:.4f}")
        
        if metrics.hallucination_scores and metrics.non_hallucination_scores:
            print(f"\nğŸ“Š åˆ†æ•°åˆ†å¸ƒ:")
            halluc_mean = statistics.mean(metrics.hallucination_scores)
            halluc_std = statistics.stdev(metrics.hallucination_scores) if len(metrics.hallucination_scores) > 1 else 0
            non_halluc_mean = statistics.mean(metrics.non_hallucination_scores)
            non_halluc_std = statistics.stdev(metrics.non_hallucination_scores) if len(metrics.non_hallucination_scores) > 1 else 0
            
            print(f"   æœ‰å¹»è§‰æ ·æœ¬åˆ†æ•°: {halluc_mean:.4f} Â± {halluc_std:.4f} (n={len(metrics.hallucination_scores)})")
            print(f"   æ— å¹»è§‰æ ·æœ¬åˆ†æ•°: {non_halluc_mean:.4f} Â± {non_halluc_std:.4f} (n={len(metrics.non_hallucination_scores)})")
    
    def run_comprehensive_evaluation(self):
        """è¿è¡Œç»¼åˆè¯„ä¼°"""
        print("ğŸš€ å¼€å§‹ RAGtruth æ•°æ®é›†ç»¼åˆè¯„ä¼°")
        
        # ä¸åŒè¯„ä¼°æ–¹æ³•çš„å¯¹æ¯”
        methods = [
            EvaluationMethod.HHEM_ONLY,
            EvaluationMethod.QWEN_ONLY,
            EvaluationMethod.ENSEMBLE
        ]
        
        results = {}
        
        for method in methods:
            print(f"\nğŸ” è¯„ä¼°æ–¹æ³•: {method.value}")
            try:
                metrics = self.evaluate_on_dataset(
                    max_samples=50,  # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨è¾ƒå°æ ·æœ¬
                    evaluation_method=method
                )
                results[method.value] = metrics
                self.print_evaluation_results(metrics)
            except Exception as e:
                print(f"âŒ è¯„ä¼°æ–¹æ³• {method.value} å¤±è´¥: {e}")
        
        # å¯¹æ¯”ä¸åŒæ–¹æ³•
        if len(results) > 1:
            print(f"\nğŸ† æ–¹æ³•å¯¹æ¯”æ€»ç»“:")
            print(f"{'æ–¹æ³•':<15} {'å‡†ç¡®ç‡':<8} {'F1åˆ†æ•°':<8} {'æˆåŠŸç‡':<8}")
            print("-" * 45)
            for method_name, metrics in results.items():
                print(f"{method_name:<15} {metrics.accuracy:<8.4f} {metrics.f1_score:<8.4f} {metrics.successful_evaluations/metrics.total_samples:<8.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®APIå¯†é’¥
    vectara_key = "zut_aE6JAgOK4H3CaDxnaz4rEfq8fI9FrYOJr_Es2g"  # ä½ çš„HHEMå¯†é’¥
    dashscope_key = None  # ä»ç¯å¢ƒå˜é‡è·å–æˆ–è®¾ç½®ä½ çš„å¯†é’¥
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = RAGtruthEvaluator(
        vectara_api_key=vectara_key,
        dashscope_api_key=dashscope_key
    )
    
    # é¦–å…ˆæŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡
    evaluator.loader.print_statistics()
    
    # è¿è¡Œè¯„ä¼°
    evaluator.run_comprehensive_evaluation()


if __name__ == "__main__":
    main()
